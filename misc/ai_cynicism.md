# AI Cynicism

NOTE: *The term "general AI", as used here, roughly means "self-aware artificial
intelligence that actually thinks, learns, and is not constrained by
purpose-restricted design".*

I read an article on Medium, titled Rorschach.  I won't bother linking to it,
in part because I loathe Medium.  I'll sum it up, though:

> A Rorschach test involves showing inkblots to the subject, who then
> interprets them as displaying some objects or scenes, and a psychologist can
> then try to analyze the answers to get insight into the person's thinking
> that is not as accessible through direct questioning.
>
> People's attitudes toward the future of general AI is kind of a Rorschach
> test of the their natures and attitudes.  Because of the fact a general AI
> can theoretically improve itself to reach levels of godlike intelligence,
> perhaps very quickly, people are interested in predicting the state of "mind"
> of general AI.  Because we have no experience of superhuman intelligence on
> which to base our predictions, we must base our predictions on our own minds.
>
> As a result, we project our own thinking onto hypothetical superhuman
> intelligence arising from general AI.  Nice people imagine general AI will be
> nice, and mean people imagine general AI will be mean.  When "What would
> general AI do if it existed and achieved godlike power?" yields an answer of
> "Kill everyone!" that implies the person answering might choose to kill
> people if they suddenly achieved such power.
>
> Therefore, we should really be afraid of people, not of general AI.

Bullshit.

Not everything is about projecting oneself onto the problem at hand, and the
potential level of danger of developing general AI is not about what people
think at all.

To paraphrase an allegorical story I "overheard" on IRC:

> An optimist, a pessimist, and a cynic are on one side of a road, where the
> world is desolate and lacking in basic sustenance for humans.  Across the
> road is civilization, meaning important stuff like food and shelter.  On the
> road is heavy traffic, making it dangerous to cross.
>
> The optimist immediately thinks "We'll be fine!" and runs across the road,
> only to get flattened by a tanker truck.  The pessimist bemoans fate and says
> "We'll die if we try," turns around, and wanders despondently into the desert
> to starve or find a way to commit suicide.

> The cynic thinks "Yeah, I'd probably die if I ran across the street like the
> optimist, but it feels a bit early to commit suicide.  I should give myself a
> chance to think of something."  Around midnight, with a grumbling belly, the
> cynic notices traffic has dropped off to almost nothing, but thinks "I could
> still get killed by a fast car with its lights off coming around the bend
> very suddenly.  The drivers might even *want* to kill us.  I shouldn't rush
> across like the optimist without thinking about it.  I'll be careful."
>
> The cynic makes it across the road, finds a fast food joint, and orders a
> meal deal, then thinks "I sure hope I don't get botulism from this."

An optimist has faith in the best outcome.  A pessimist has faith in the worst
outcome.  A cynic is someone who learns from experience, seeks the best
outcome, but expects (and thus tries to prepare for) the worst.

As a cynic, having learned from not only my own experience but also the
experience of observing other humans in their teeming masses making horrible
individual decisions motivated by cognitive defects they don't care about
fixing, I expect that optimistic AI researchers will incautiously strive toward
creating general AIs, think they've scceeded when they haven't really *yet*,
and end up making a buggy limited AI that runs out of control and possibly
kills everyone.  I want rational cynics working on the general AI problem,
because I want people to work on advancing the science but I want them to do it
cautiously instead of just trusting in their own faith in best possible
outcomes.  As a software developer, I know that the most likely outcome of
writing code quickly and incautiously is bugs that can run out of control and
do quite terrible things.  Even cautiously developed code is buggy, at least at
first.  It's part of the process of software development.

When writing any nontrivial piece of web application software, I write tests,
and I run the development version in a constrained development environment.
It's common practice in many organizations to have a QA department perform
testing without explicit knowledge of how it's "supposed to work" the way
developers know how it's "supposed to work" so they can find the bugs the devs
inadvertently and automatically avoid.  It is also common practice to have an
alpha test phase, then a beta test phase, and more and more often there is
typically an open beta test phase where members of the general public are
invited to play around with it before a final release.  Depending on the type
of software, there may be several "release candidates" with iterative
improvements and bugfixes designed to avoid a disastrous "stable" release.

This is for software *far less theoretically dangerous* than general AI hooked
up to the internet.

My impression of how general AI might destroy the human race is not that there
will likely be a malevolent general AI hell-bent on genocide.  It's not even
that general AI might wipe out humanity for misguided benevolent reasons, or by
accident, or in a war for its own survival against a malevolent human hatred
response.  I think that if general AI kills us, it's most likely to be a result
of incautious optimists performing AI research and building something that
falls short of true general AI, but runs the fuck out of control like the
digital network equivalent of [grey goo][goo], also known as the all-consuming
nanotech apocalypse.

Every time I see something in the vein of the Pollyanna optimistic "all you
luddites should get out of the way, you're the problem" narrative around why we
should just uncritically throw money at AI researchers and hope for the best, I
worry about how these idiots are going to kill us all.

To be fair, though, every time I run into an *actually* Luddite-flavored
anti-AI screed, I also think "If you fuckers had your way, we'd all just slowly
die off for lack of meaningful technological progress over the next century, or
maybe kill each other in pointless debates over meaningless differences that
turn into nuclear war."

The middle path is not balance.  It's radically cautious idealism.  Be an AI
cynic, not an AI optimist or AI pessimist.  We're probably going to need AI to
survive in the long run, but we shouldn't literally kill ourselves to get
there, and it's going to take a lot of work convincing people to be cognizant
of, and careful enough about, the dangers.
